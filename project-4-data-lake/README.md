# Project-4-Data-Lake. This ingestion is based on the files that come on the S3 bucket, and the code to load the files to S3 bucket.

Let's get started with this code

## Table of Contents

1. [Quick start](#quick-start)
1. [Song Dataset](#Song-Dataset)
1. [Log Dataset](#Log-Dataset)
2. [Generate docker image](#Generate-docker-image)   
2. [Run datalake ingestion](#Run-Pex-datalake-ingestion)
2. [Prepare the environment](#Prepare-the-environment)

## Summary

# Project: Data Lake

Introduction
A music streaming startup, Sparkify, has grown their user base and song database even more and his old data warehouse was migrated to a data lake. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.
So then, the data pipeline was constructed with the purpose of modeling and automate the data extractions, in a more flexible and powerfull way.

Project Description
In this project, I made tasks on Spark and data lakes to build an ETL pipeline and the result is a data lake hosted on S3. 
The process steps consists in:
* Load data from S3
* Process the data into analytics tables using Spark
* load them back into S3

## Quick start

1. The purpose here, is to load some songs data, thinking in a self-service architecture, to build an ingestion process based on the data loaded before.

Notes:

We have 2 datasets here:

## Song Dataset
The first dataset is a subset of real data from the https://labrosa.ee.columbia.edu/millionsong/. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

## Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.


```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```


And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.


| artist - auth - firstName - gender - itemInSession - lastName - length - level - location - method - page - registration - sessionId - song - status - ts -  userAgent -  userId |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------|

```
Sydney Youngblood Logged In Jacob M 53 Klein 238.07955 paid Tampa-St. Petersburg-Clearwater, FL PUT NextSong 1.540558e+12 954 Ain't No Sunshine 200 1543449657796 "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4... 73 1 Gang Starr Logged In Layla F 88 Griffin 151.92771 paid Lake Havasu City-Kingman, AZ PUT NextSong 1.541057e+12 984 My Advice 2 You (Explicit) 200 1543449690796 "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK... 24 2 3OH!3 Logged In Layla F 89 Griffin 192.52200 paid Lake Havasu City-Kingman, AZ PUT NextSong 1.541057e+12 984 My First Kiss (Feat. Ke$ha) [Album Version] 200 1543449841796 "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK... 24 3 RÃ�Â¶yksopp Logged In Jacob M 54 Klein 369.81506 paid Tampa-St. Petersburg-Clearwater, FL PUT NextSong 1.540558e+12 954 The Girl and The Robot 200 1543449895796 "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4... 73 4 Kajagoogoo Logged In Layla F 90 Griffin 223.55546 paid Lake Havasu City-Kingman, AZ PUT NextSong 1.541057e+12 984 Too Shy 200 1543450033796 "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK... 24

```

2. Datalake Ingestion

The ingestion code, is responsible to build the datalake.

* You need to have docker installed on your machine.
* follow the instructions in this link - https://docs.docker.com/engine/install/

* Run instructions
So, to run the code, you will need to follow these steps:
## Generate docker image
1) 
```
   $ make first_build
```
## Run Pex datalake ingestion
2) After, you execute as below:
```
Go to the first line of Makefile and change the name of aws_profile or set your profile name to
udacity
And then, execute the command below:
    $ make deploy_aws
    
This command, will Do:
1) Push the generated image to aws ecr
2) Copy pex to the S3 bucket
3) Create the EMR instance and run the spark job
```
    
# If you have any problems with the execution, try the following requirements(on linux ubuntu distros):

## Prepare the environment

```
#install docker - linux ubuntu distros (linux mint in this case)

# step 1
sudo apt-get update

sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg \
    lsb-release

#step 2
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg

echo   "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu focal stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

sudo apt-get update

#step 3

sudo apt-get update
sudo apt-get install docker-ce docker-ce-cli containerd.io

#testing install

sudo docker run hello-world

```

## End of README
